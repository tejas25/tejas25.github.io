<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="stylesheet" href="/about/css/bootstrap.min.css">
  
  
  <title>Analysis of Boston Property Assessments | Tejas Rawal</title>
  <style>
.container {
  max-width: 700px;
}
#nav a {
  font-weight: bold;
  color: inherit;
}
#nav a.nav-link-active {
  background-color: #212529;
  color: #fff;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
</style>
</head>
  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav id="nav" class="nav justify-content-center">
  
  
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/about/"><i data-feather="home"></i> Home</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/about/post/"><i data-feather="edit"></i> Projects</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/about/tags/"><i data-feather="tag"></i> Tags</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/about/about/"><i data-feather="smile"></i> About</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/about/cv/cv_2019.pdf"><i data-feather="rss"></i> CV</a>
  
  </nav>
</div>
    <div class="container">
      <main id="main">
        

<h1>Analysis of Boston Property Assessments</h1>


<i data-feather="calendar"></i> <time datetime="2018-12-07">Dec 7, 2018</time>

  <br>
  <i data-feather="tag"></i>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="/about/tags/boston">Boston</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="/about/tags/property-assessment">Property Assessment</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="/about/tags/machine-learning">Machine Learning</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="/about/tags/ensemble-learner">Ensemble Learner</a>
  

<br><br>



<hr />
<pre class="r"><code>library(&quot;tidyverse&quot;)
library(&quot;caret&quot;)
library(&quot;neuralnet&quot;)
library(&quot;rpart&quot;)
library(&quot;rpart.plot&quot;)
library(&quot;caretEnsemble&quot;)</code></pre>
<hr />
<div id="business-understanding-introduction" class="section level1">
<h1>Business Understanding : Introduction</h1>
<p>A recent article dated Nobember 28, 2018 from <strong><a href="https://www.bostonglobe.com/business/2018/11/28/there-are-signs-that-boston-area-heated-housing-market-cooling/OlvkEOZ9rmLJIHvDOvOTpK/story.html">Boston Globe</a></strong> suggests that <em>“Boston is still a tough - and expensive - place to buy a house, but the years of prices galloping relentlessly upward may be nearing an end.”</em> According to data from the Greater Boston Association of Realtors the number of homes and condominiums sold in Boston has increased 6 percent in July 2018 compared to July 2017. The volume of sales is slowing down inspite of newly listed homes have been increasing since last four months. However the properties with large areas, better eminities and viwes are in great demand and are sold at very high prices. With this background this exploratory research is focused on analysing the Boston property trends and make models to predict property values which can be helpful to sellers and buyers of Boston.</p>
<p>This assessment is carried out using the <strong><em><a href="https://exde.files.wordpress.com/2009/03/crisp_visualguide.pdf">CRISP-DM</a></em></strong> methodology. CRISP-DM stands for Cross-Industry Process for Data Mining. The CRISP-DM methodology employs a structured approach to planning a data mining project.</p>
<p>The objective of this project is to explore the <strong><em><a href="https://data.boston.gov/dataset/property-assessment/resource/fd351943-c2c6-4630-992d-3f895360febd">Boston Property Assessment</a></em></strong> 2018 data, understand trends in property values within Boston and predict their property values based on the recorded charisteristics in this assessment dataset.</p>
<p>Insightful findings for Boston real estate are analyzed using open source Property Assessment data from the City of Boston for year 2018. Also machine learning algorithms have been applied to try and predict property values across the city.</p>
<p>The dataset along with data key is available at : <a href="https://data.boston.gov/dataset/property-assessment" class="uri">https://data.boston.gov/dataset/property-assessment</a></p>
</div>
<div id="data-understanding" class="section level1">
<h1>Data Understanding</h1>
<p>The 2018 dataset has 75 features and 172841 records of property parcels. Each observation represented a single property, and the features included characteristics about the property such as the location (zip code, street, unit number), the taxable value, square footage, year built/remodeled, and the condition, among many others.</p>
<p>It has various categorical, string and numeric features, which will be appropriately standardized depending on the machine learning algorithm. Apart from missing values the dataset is not too messy which reduces the hassle of tidying process.</p>
<pre class="r"><code># reading the dataset in
property &lt;- read.csv(&quot;ast2018full.csv&quot;, header = T)

# converting all features to lower case
names(property) &lt;- tolower(names(property))</code></pre>
<div id="types-of-properties" class="section level2">
<h2>Types of Properties</h2>
<p>The variable <code>ptype</code> represents the “PROPERTY OCCUPANCY CODES”. The codes and their description can explored from the <strong><em><a href="https://data.boston.gov/dataset/property-assessment/resource/d6c1268c-cd83-4dc3-a914-bba1ed59da6d">occupancy code sheet</a></em></strong>.</p>
<p>Properties like residential, apartment, commercial, multiple-use, industrial, ownership-exempt, tax-exempt and their subcategories are number coded in this variable. With the help of few codes shown below, frequencies of top 10 property occupancy types can be identified.</p>
<pre class="r"><code># extracting property occupancy codes frequencies
top_prop &lt;- as.data.frame(table(property$ptype))

# renaming the columns
colnames(top_prop) &lt;- c(&quot;ptype&quot;, &quot;Freq&quot;)

# extracting top 10 property types based on frequency
top_prop &lt;- top_prop %&gt;% arrange(desc(Freq)) %&gt;% head(10)
occu_code &lt;- c(&quot;Residential Condo&quot;, &quot;Single Fam Dwelling&quot;, &quot;Two-Fam Dwelling&quot;, 
    &quot;Three-Fam Dwelling&quot;, &quot;Tax Exmp Condo Main&quot;, &quot;Condo Parking&quot;, 
    &quot;Vacant Land&quot;, &quot;Oth Exmp Bldg&quot;, &quot;Apt 4-6 Units&quot;, &quot;Res/Commercial Use&quot;)
top_prop &lt;- cbind(top_prop, occu_code)
top_prop[, c(3, 1, 2)]</code></pre>
<pre><code>##              occu_code ptype  Freq
## 1    Residential Condo   102 62633
## 2  Single Fam Dwelling   101 30567
## 3     Two-Fam Dwelling   104 17302
## 4   Three-Fam Dwelling   105 13706
## 5  Tax Exmp Condo Main   995  9622
## 6        Condo Parking   108  5479
## 7          Vacant Land   132  4673
## 8        Oth Exmp Bldg   985  2641
## 9        Apt 4-6 Units   111  2576
## 10  Res/Commercial Use    13  1847</code></pre>
<pre class="r"><code># calculating percent share of top 4 property types
top_prop %&gt;% arrange(desc(Freq)) %&gt;% filter(Freq &gt; 10000) %&gt;% 
    summarise(top_4_total = sum(Freq), top_4_share = sum(Freq)/nrow(property), 
        Condo_share = 62633/nrow(property))</code></pre>
<pre><code>##   top_4_total top_4_share Condo_share
## 1      124208   0.7186258   0.3623735</code></pre>
<p>About <strong>72 percent</strong> of the properties consists of <strong>top 4 property type</strong> frequencies which are “Residential Condo”, “Single Fam Dwelling”, “Two-Fam Dwelling”, and “Three-Fam Dwelling” and <strong>“Residential Condo”</strong> alone accounts for <strong>36.23 percent</strong>. With this finding we can narrow down our analysis and focus on Condo property types.</p>
<pre class="r"><code>top_prop %&gt;% ggplot(aes(reorder(occu_code, -Freq), Freq, fill = occu_code)) + 
    geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = &quot;Top 10 Property Type Frequency&quot;, 
    x = &quot;Property Type&quot;, y = &quot;Frequency&quot;)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<blockquote>
<p>The analysis will be focused only to “Residential Condo” units listed in the dataset</p>
</blockquote>
</div>
<div id="residential-properties" class="section level2">
<h2>Residential Properties</h2>
<p>The new dataset now can be extracted based on this selected property type “condo”. The reason behind selecting only the residential condo is, other property types are relatively fewer in number which might not be sufficient enough for model building and testing process. And also the feature selection becomes easier for one type of property compared to many.</p>
<pre class="r"><code># extracting property type of &#39;Residential Condo&#39; from the
# dataset
property_resi &lt;- property %&gt;% filter(ptype == 102)

property_resi$u_tot_rms &lt;- as.numeric(property_resi$u_tot_rms)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-6-1.png" width="624" /></p>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-7-1.png" width="624" /></p>
<p><a name="pookie"></a></p>
</div>
<div id="variable-devision" class="section level2">
<h2>Variable Devision</h2>
<ul>
<li><p>1 to 16 variables are property identification variables<br />
<font size="1">pid, cm_id, gis_id, st_num, st_name, st_name_suf, unit_num, zipcode, ptype, lu, own_occ, owner, mail_addressee, mail_address, mail.cs, mail_zipcode</font></p></li>
<li><p>17 to 27 variables are property values, areas, built year etc.<br />
<font size="1"> av_land, av_bldg, av_total, gross_tax, land_sf, yr_built, yr_remod, gross_area, living_area, num_floors, structure_class</font></p></li>
<li><p>28 to 49 variables are “residential” property characteristics<br />
<font size="1"> r_bldg_styl, r_roof_typ, r_ext_fin, r_total_rms, r_bdrms, r_full_bth, r_half_bth, r_bth_style, r_bth_style2, r_bth_style3,r_kitch, r_kitch_style, r_kitch_style2, r_kitch_style3,r_heat_typ, r_ac, ,r_fplace, r_ext_cnd, r_ovrall_cnd,r_int_cnd, r_int_fin, r_view</font></p></li>
<li><p>50 to 56 variables are “Condo Main” property characteristics<br />
<font size="1"> s_num_bldg, s_bldg_styl, s_unit_res, s_unit_com, s_unit_rc, s_ext_fin, s_ext_cnd </font></p></li>
<li><p>57 to 75 variables are “Appartment/Condo” property characteristics<br />
<font size="1"> u_base_floor, u_num_park, u_corner, u_orient, u_tot_rms, u_bdrms, u_full_bth, u_half_bth, u_bth_style, u_bth_style2, u_bth_style3, u_kitch_type, u_kitch_style, u_heat_typ, u_ac, u_fplace, u_int_fin, u_int_cnd, u_view</font></p></li>
</ul>
</div>
<div id="explore-data" class="section level2">
<h2>Explore Data</h2>
<p>All variables are not useful for understanding the trend and patterns of property valuation. Few of the identification variables are summarised below to understand the distribution, probable outliers, and NA’s. Histograms and correlation matrix for these identification variables are also prepared to see the distribution and relation between these variables.</p>
<pre class="r"><code>summary(property_resi[c(9, 11, 19, 20, 21, 22, 25, 26)])</code></pre>
<pre><code>##      ptype     own_occ      av_total          gross_tax       
##  Min.   :102    :    0   Min.   :   30100   Min.   :   31545  
##  1st Qu.:102   N:30263   1st Qu.:  319500   1st Qu.:  334836  
##  Median :102   Y:32370   Median :  458700   Median :  480718  
##  Mean   :102             Mean   :  642470   Mean   :  673309  
##  3rd Qu.:102             3rd Qu.:  683800   3rd Qu.:  716623  
##  Max.   :102             Max.   :33727536   Max.   :35346458  
##                                                               
##     land_sf         yr_built     living_area      num_floors   
##  Min.   :   23   Min.   :   0   Min.   :    0   Min.   :1.000  
##  1st Qu.:  694   1st Qu.:1900   1st Qu.:  693   1st Qu.:1.000  
##  Median :  931   Median :1920   Median :  928   Median :1.000  
##  Mean   : 1055   Mean   :1934   Mean   : 1040   Mean   :1.263  
##  3rd Qu.: 1259   3rd Qu.:1985   3rd Qu.: 1244   3rd Qu.:1.000  
##  Max.   :13943   Max.   :2016   Max.   :13943   Max.   :7.000  
##  NA&#39;s   :6       NA&#39;s   :36     NA&#39;s   :36      NA&#39;s   :36</code></pre>
<pre class="r"><code>par(mfrow = c(3, 3))
hist(property_resi$av_bldg)
hist(property_resi$av_total)
hist(property_resi$gross_tax)
hist(property_resi$land_sf)
hist(property_resi$yr_built)
hist(property_resi$living_area)
hist(property_resi$num_floors)
hist(property_resi$u_tot_rms)
hist(property_resi$u_bdrms)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Correlation matrix
# psych::pairs.panels(property_resi[c(19,20,21,22,25,26)])
# output not created because it is time consuming and results
# in to very large file size

cor(as.matrix(property_resi[c(19, 20, 21, 22, 25, 26)]), use = &quot;pairwise.complete.obs&quot;)</code></pre>
<pre><code>##               av_total  gross_tax   land_sf   yr_built living_area
## av_total    1.00000000 1.00000000 0.6741257 0.08112795   0.7110513
## gross_tax   1.00000000 1.00000000 0.6741258 0.08112794   0.7110513
## land_sf     0.67412572 0.67412576 1.0000000 0.17712800   0.9791355
## yr_built    0.08112795 0.08112794 0.1771280 1.00000000   0.1726956
## living_area 0.71105125 0.71105130 0.9791355 0.17269556   1.0000000
## num_floors  0.12403363 0.12403367 0.4992223 0.10701220   0.4779372
##             num_floors
## av_total     0.1240336
## gross_tax    0.1240337
## land_sf      0.4992223
## yr_built     0.1070122
## living_area  0.4779372
## num_floors   1.0000000</code></pre>
<p>The variables land_sf,yr_built and living_area have <code>NA</code>. Also the variables land_sf, living_area, av_total, gross_tax have very large diffrence between maximum and 3rd quantile values, which shows that the are few outliers in the dataset. Variables av_total and gross_tax are highly correlated which mean we can drop the gross tax variable data analysis. Also from the graph below we see that there are outliers in the av_total variable.</p>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-9-1.png" width="624" /></p>
</div>
</div>
<div id="data-preparation" class="section level1">
<h1>Data Preparation</h1>
<div id="clean-and-imputation-data" class="section level2">
<h2>Clean and Imputation Data</h2>
<p>With above finding, the removal of outliers can be carried out. The outlier criteria for yr_built is <code>0</code> and <code>NA</code>. The outliers criteria for av_total is property value above <code>1 mil</code> dollars. The observations with these outlier criteria are <code>71</code>. The reason for removing these is to not influance the model prediction by these high-end and expensive properties. Also there are about 6 <code>NA</code> in the land_sf variable which can be imputed with its mean.</p>
<pre class="r"><code># identifying yr_built and av_total outliers
property_resi %&gt;% select(yr_built, av_total) %&gt;% filter(yr_built %in% 
    c(0, NA) | av_total &gt; 1e+07) %&gt;% count()</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1    71</code></pre>
<pre class="r"><code># removing yr_built and av_total outliers
property_resi &lt;- property_resi %&gt;% filter(!yr_built %in% c(0, 
    NA)) %&gt;% filter(av_total &lt; 1e+07)


# imputing NA in land area with mean
property_resi$land_sf &lt;- ifelse(is.na(property_resi$land_sf) == 
    T, mean(property_resi$land_sf, na.rm = T), property_resi$land_sf)</code></pre>
</div>
<div id="principal-component-analysis" class="section level2">
<h2>Principal Component Analysis</h2>
<p>Apart from the variabes discussed in <strong>Explore Data</strong> section above, the bumeric variables in the data should be analysed with the help of Principal component analysis for exploratory data analysis, allowing to better visualize the variation present in dataset with many variables. It is particularly helpful in the case of wide datasets like this that we are dealing with.</p>
<pre class="r"><code># Extracting numeric features
nums &lt;- unlist(lapply(property_resi, is.numeric))

# Performing PCA
prop_pca &lt;- prcomp(property_resi[c(19, 20, 21, 22, 25, 26)], 
    center = TRUE, scale. = TRUE)

summary(prop_pca)</code></pre>
<pre><code>## Importance of components%s:
##                          PC1    PC2    PC3     PC4     PC5       PC6
## Standard deviation     1.885 1.0743 0.9585 0.59511 0.14377 3.574e-07
## Proportion of Variance 0.592 0.1924 0.1531 0.05903 0.00344 0.000e+00
## Cumulative Proportion  0.592 0.7844 0.9375 0.99656 1.00000 1.000e+00</code></pre>
<p>The variables av_total, gross_tax and land_sf explain respectively 59.2 percent , 19.2 percent and 15.3 percent of the total variance in the dataset (6 numeric variables).</p>
</div>
<div id="select-data" class="section level2">
<h2>Select Data</h2>
<p>This filtered dataset has basically 2 divisions in the data structure. <strong>1. Residential Condo</strong> and <strong>2. 1/2/3 Family Dwelling</strong>. As discussed in <a href="#pookie">Section 2.1</a> the dataset has saperate variables for residential properties(1/2/3 family dwelling units) and saperate variables for appartment/condo properties. Therefore the logical thing to do would be to split the dataset into two and select the data only relevant to Condos. This will help to remove many empty fields from the dataset.</p>
<pre class="r"><code># renaming the dataset
condo &lt;- property_resi</code></pre>
<p>For selecting data the same criteria is applied, as discussed in previous <a href="#pookie">Section 2.1</a>. Additionally property identification variables like <em>“cm_id”,“gis_id”, “st_num”, “st_name_suf”,“unit_num”, “lu”, “owner”, “mail_addressee”, “mail_address”, “mail.cs”, and “mail_zipcode”</em> can be omited from the data set because they are not useful for analysis.</p>
<pre class="r"><code>condo &lt;- condo %&gt;% select(&quot;zipcode&quot;, &quot;own_occ&quot;, &quot;av_total&quot;, &quot;land_sf&quot;, 
    &quot;yr_built&quot;, &quot;yr_remod&quot;, &quot;gross_area&quot;, &quot;living_area&quot;, &quot;num_floors&quot;, 
    &quot;u_base_floor&quot;, &quot;u_corner&quot;, &quot;u_orient&quot;, &quot;u_tot_rms&quot;, &quot;u_bdrms&quot;, 
    &quot;u_full_bth&quot;, &quot;u_bth_style&quot;, &quot;u_kitch_type&quot;, &quot;u_kitch_style&quot;, 
    &quot;u_heat_typ&quot;, &quot;u_ac&quot;, &quot;u_int_fin&quot;, &quot;u_int_cnd&quot;, &quot;u_view&quot;)


# removing a row with missing values in almost all
# characteristics variabes
condo &lt;- condo[-35173, ]

# readjusting factor levels for few variables
condo[] &lt;- lapply(condo, function(x) if (is.factor(x)) factor(x) else x)
levels(condo$u_corner)[1] &lt;- &quot;N&quot;
condo$u_orient &lt;- plyr::revalue(condo$u_orient, c(a = &quot;A&quot;, A = &quot;A&quot;, 
    B = &quot;B&quot;, C = &quot;C&quot;, E = &quot;E&quot;, f = &quot;F&quot;, F = &quot;F&quot;, m = &quot;M&quot;, M = &quot;M&quot;, 
    t = &quot;T&quot;, T = &quot;T&quot;))
condo$u_heat_typ &lt;- plyr::revalue(condo$u_heat_typ, c(` ` = &quot;O&quot;, 
    C = &quot;O&quot;, D = &quot;O&quot;, E = &quot;E&quot;, f = &quot;F&quot;, F = &quot;F&quot;, G = &quot;O&quot;, I = &quot;O&quot;, 
    N = &quot;N&quot;, P = &quot;P&quot;, S = &quot;S&quot;, w = &quot;W&quot;, Y = &quot;W&quot;))
condo$u_int_fin &lt;- plyr::revalue(condo$u_int_fin, c(E = &quot;E&quot;, 
    n = &quot;N&quot;, S = &quot;S&quot;))</code></pre>
<p>With this selection process the dataset end up with 30 relavent variables.</p>
</div>
<div id="construct-data" class="section level2">
<h2>Construct Data</h2>
<p>Here it would be intresting to know the effect of remodeling the house (or part of the house) on property assessment value. To incorporate this in to our analysis, duration from the <strong>year of remodel to year “2018” diffrence</strong> for all the observations can be calculated. This diffrence can be added as a new variable to <strong>condo</strong> datsets. Once this new column is added we can drop the <code>yr_remod</code> variable column from both datasets, to eliminate <code>NA</code>.</p>
<pre class="r"><code># creating new column with if condition to calculate years
# since remodel
condo$remod_diff &lt;- ifelse(condo$yr_remod %in% c(0, 1500, NA), 
    2018 - condo$yr_built, 2018 - condo$yr_remod)

# dropping the variable yr_remod from both condo datsets
condo &lt;- condo %&gt;% select(-yr_remod)</code></pre>
</div>
<div id="integrate-data" class="section level2">
<h2>Integrate Data</h2>
<p>An intresting aspect of analysis would be to look at the property prices, “zipcode” wise. The zip codes are not very efficient for understanding the property values and compare it with other parts of the city. Here respective neighbourhood names can be added to the datasets in new column to understand to add the geographic perspective in the analysis. The relavent neighborhood names present in our dataset have been taken from <strong><a href="http://archive.boston.com/news/local/articles/2007/04/15/sixfigurezipcodes_city/">here</a></strong>.</p>
<pre class="r"><code># creating data frame for neighbourhood with zipcodes
nbhd &lt;- data.frame(zipcode = c(2108, 2109, 2110, 2111, 2113, 
    2114, 2115, 2116, 2118, 2119, 2120, 2121, 2122, 2124, 2125, 
    2126, 2127, 2128, 2129, 2130, 2131, 2132, 2134, 2135, 2136, 
    2199, 2210, 2215, 2446, 2467, 2186, 2445), nbh = c(&quot;Beacon  Hill&quot;, 
    &quot;North End&quot;, &quot;Downtown&quot;, &quot;Chinatown&quot;, &quot;Hanover&quot;, &quot;West End&quot;, 
    &quot;Fenway&quot;, &quot;Back Bay&quot;, &quot;South End&quot;, &quot;Roxbury&quot;, &quot;Mission Hill&quot;, 
    &quot;Grove Hall&quot;, &quot;Dorchester&quot;, &quot;Dorchester&quot;, &quot;Dorchester&quot;, &quot;Mattapan&quot;, 
    &quot;South Boston&quot;, &quot;East Boston&quot;, &quot;Charlestown&quot;, &quot;Jamaica Plain&quot;, 
    &quot;Roslindale&quot;, &quot;West Roxbury&quot;, &quot;Allston&quot;, &quot;Brighton&quot;, &quot;Hyde Park&quot;, 
    &quot;Boylston&quot;, &quot;Boston&quot;, &quot;Kenmore&quot;, &quot;Brookline&quot;, &quot;Brookline&quot;, 
    &quot;Milton&quot;, &quot;Brookline&quot;))

# adding neighbourhood column to Condo condo &lt;-
# left_join(condo, nbhd, by = &#39;zipcode&#39;)</code></pre>
<p>Note: later on realised that integration will not have any use.</p>
</div>
<div id="checking-for-na" class="section level2">
<h2>Checking for <code>NA</code></h2>
<p>Now that the dataset is ready, it would be wise to once again check for any <code>NA</code> values that might have been missed and also have a look at summary of the datasets.</p>
<pre class="r"><code>sum(is.na(condo))</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>summary(condo)</code></pre>
<pre><code>##     zipcode     own_occ      av_total          land_sf         yr_built   
##  Min.   :2108   N:30200   Min.   :  30100   Min.   :   23   Min.   :1791  
##  1st Qu.:2116   Y:32361   1st Qu.: 319700   1st Qu.:  694   1st Qu.:1900  
##  Median :2127             Median : 458700   Median :  930   Median :1920  
##  Mean   :2132             Mean   : 637788   Mean   : 1053   Mean   :1934  
##  3rd Qu.:2130             3rd Qu.: 683600   3rd Qu.: 1258   3rd Qu.:1985  
##  Max.   :2467             Max.   :9830000   Max.   :13943   Max.   :2016  
##                                                                           
##    gross_area     living_area      num_floors     u_base_floor   
##  Min.   :    0   Min.   :    0   Min.   :1.000   Min.   : 0.000  
##  1st Qu.:  694   1st Qu.:  693   1st Qu.:1.000   1st Qu.: 1.000  
##  Median :  933   Median :  927   Median :1.000   Median : 2.000  
##  Mean   : 1054   Mean   : 1038   Mean   :1.263   Mean   : 3.335  
##  3rd Qu.: 1260   3rd Qu.: 1243   3rd Qu.:1.000   3rd Qu.: 3.000  
##  Max.   :13943   Max.   :13943   Max.   :7.000   Max.   :59.000  
##                                                                  
##  u_corner  u_orient    u_tot_rms        u_bdrms         u_full_bth    
##  N:50878   A: 9020   Min.   : 2.00   Min.   : 0.000   Min.   : 0.000  
##  Y:11683   B: 1091   1st Qu.:18.00   1st Qu.: 1.000   1st Qu.: 1.000  
##            C: 1638   Median :20.00   Median : 2.000   Median : 1.000  
##            E:  141   Mean   :19.71   Mean   : 1.767   Mean   : 1.335  
##            F:15043   3rd Qu.:21.00   3rd Qu.: 2.000   3rd Qu.: 2.000  
##            M: 2167   Max.   :27.00   Max.   :17.000   Max.   :20.000  
##            T:33461                                                    
##  u_bth_style u_kitch_type u_kitch_style u_heat_typ u_ac      u_int_fin
##  L: 3672     F:21333      L: 3893       O:   37    C:32849   E: 2666  
##  M:29894     N:  116      M:31586       E: 4733    D:  116   N:59836  
##  N: 1601     O:38337      N: 1538       F:24541    N:29596   S:   59  
##  S:27394     P: 2775      S:25544       N:    4                       
##                                         P: 4071                       
##                                         S:   96                       
##                                         W:29079                       
##  u_int_cnd u_view      remod_diff    
##  A:24684   A:47893   Min.   :  1.00  
##  E: 6836   E: 3331   1st Qu.: 12.00  
##  F:  239   F: 2960   Median : 23.00  
##  G:30776   G: 7984   Mean   : 27.64  
##  P:   26   P:  197   3rd Qu.: 35.00  
##            S:  196   Max.   :190.00  
## </code></pre>
</div>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p>The models considered here for predicting the assessment value of condo properties in Boston are <strong>linear regression</strong>, <strong>regression tree</strong> and <strong>neural network</strong>. Befor starting the modeling process the data has to be processed, mainy in terms normaliztion and creating dumy variables.</p>
<div id="data-processing" class="section level2">
<h2>Data processing</h2>
<p>With the help of function <code>dummyVars</code> in <code>caret</code> package creating dummy variables is very easy. The general rule for creating dummy variables is to have one less variable than the number of categories present to avoid perfect collinearity. the command <code>fullRank</code> in this function helps to solve this problem of collinearity by creating one less variable of all categories.</p>
<pre class="r"><code># Loading the required libraries library(&#39;caret&#39;)

# Seeting the random seed
set.seed(25)


# creating dummy variables for all the categorical variables
dmy &lt;- dummyVars(&quot; ~ .&quot;, data = condo, fullRank = T)
condo_dmy &lt;- data.frame(predict(dmy, newdata = condo))
dim(condo_dmy)</code></pre>
<pre><code>## [1] 62561    48</code></pre>
<p>This process creates extra variables in the dataset and now the original Condo dataset has 48 variables.</p>
</div>
<div id="feature-selection" class="section level2">
<h2>Feature selection</h2>
<p>Now that the dummy variables have been created, it is not wise to use all the variables unless explicitly needed. Therefore ratherethan getting into complicated feature selection methos, the variables for model building are selected based on their correlation with the assessment value variable</p>
<pre class="r"><code># finding correlation of variables with av_total
correl &lt;- data.frame(assment.correlation = cor(condo_dmy[, -3], 
    condo_dmy[, 3]))

# identify variables with correlation grater than +-0.25
correl &lt;- correl %&gt;% round(2) %&gt;% rownames_to_column(&quot;variables&quot;) %&gt;% 
    arrange(desc(assment.correlation)) %&gt;% filter(assment.correlation &gt;= 
    0.25 | assment.correlation &lt;= -0.25)

# create vector of correlated variable
vari &lt;- correl %&gt;% pull(variables)
vari &lt;- c(vari, &quot;av_total&quot;)</code></pre>
</div>
<div id="normalization" class="section level2">
<h2>Normalization</h2>
<p>Normalizing the data is very important before it is used in any process of modeling. Using the <code>caret</code> package again the preprocessing of the data is carried out as shown below. The methodes used are “center” and “scale”, which basically means <strong>Z-score</strong> normalization using mean and standard deviation of the columns. Here the normalization process is only done for the selected variables identified in above feature selecction phase.</p>
<pre class="r"><code># data normalization for correlated variables
preprocessParams &lt;- preProcess(condo_dmy[, vari], method = c(&quot;center&quot;, 
    &quot;scale&quot;))
transf &lt;- predict(preprocessParams, condo_dmy[, vari])</code></pre>
</div>
<div id="spiting-the-data" class="section level2">
<h2>Spiting the Data</h2>
<p>The data set is split into traing dataset and test dataset using <code>createDataPartition</code> handy function from <code>caret</code>. the split is made in proportion of 75% - 25% for training and test dataset respectively.</p>
<pre class="r"><code># spliting training set into two parts based on outcome: 75%
# and 25%
index &lt;- createDataPartition(transf$av_total, p = 0.75, list = FALSE)
trainSet &lt;- transf[index, ]
testSet &lt;- transf[-index, ]</code></pre>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple Linear Regression</h2>
<p>Multiple linear regression is by far the most comman method for modeling and predicting numeric data.</p>
<pre class="r"><code># Training the regression model reg_model &lt;-
# lm(trainSet$av_total~.,trainSet)
reg_model &lt;- step(lm(trainSet$av_total ~ ., trainSet), direction = &quot;backward&quot;, 
    trace = F)

summary(reg_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = trainSet$av_total ~ living_area + land_sf + gross_area + 
##     u_full_bth + u_int_cnd.E + u_base_floor + u_view.E + u_view.S + 
##     u_bdrms + u_bth_style.S + u_kitch_style.S + u_ac.N + u_int_fin.N, 
##     data = trainSet)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.3746  -0.2403   0.0053   0.2482   8.7328 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      0.0004289  0.0025231   0.170 0.865025    
## living_area      1.0365538  0.0140320  73.871  &lt; 2e-16 ***
## land_sf          0.0815403  0.0247081   3.300 0.000967 ***
## gross_area      -0.5030221  0.0265973 -18.913  &lt; 2e-16 ***
## u_full_bth       0.1043206  0.0036879  28.287  &lt; 2e-16 ***
## u_int_cnd.E      0.1435553  0.0030448  47.148  &lt; 2e-16 ***
## u_base_floor     0.0409252  0.0034765  11.772  &lt; 2e-16 ***
## u_view.E         0.1308474  0.0031961  40.940  &lt; 2e-16 ***
## u_view.S         0.1483806  0.0027752  53.466  &lt; 2e-16 ***
## u_bdrms         -0.1970060  0.0033707 -58.447  &lt; 2e-16 ***
## u_bth_style.S   -0.0086603  0.0047964  -1.806 0.070992 .  
## u_kitch_style.S -0.0331833  0.0047626  -6.968 3.27e-12 ***
## u_ac.N           0.0533807  0.0029396  18.159  &lt; 2e-16 ***
## u_int_fin.N     -0.1673725  0.0030134 -55.543  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5465 on 46908 degrees of freedom
## Multiple R-squared:  0.7052, Adjusted R-squared:  0.7051 
## F-statistic:  8630 on 13 and 46908 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># Predicting for test dataset
testSet$pred_lr &lt;- predict(object = reg_model, testSet[, c(1:13)])

# accuracy
cor(testSet$pred_lr, testSet$av_total)</code></pre>
<pre><code>## [1] 0.8440261</code></pre>
<pre class="r"><code># Root mean squared error
RMSE(testSet$pred_lr, testSet$av_total)</code></pre>
<pre><code>## [1] 0.5259863</code></pre>
<p>The model summary states that the R-squared is 0.71 and the overall model is statistically significant as the p-value is well below 0.05. Almost all independent variables are also statistically significant. Residual standard error is 0.55. This model when used on the test dataset the predictions of av_total is having correlation of <span class="math inline">\(0.84\)</span> and root mean squared error <span class="math inline">\(0.53\)</span>.</p>
</div>
<div id="regression-tree-model" class="section level2">
<h2>Regression Tree Model</h2>
<pre class="r"><code># Loading the required libraries library(&#39;rpart&#39;)
# library(&#39;rpart.plot&#39;)


# training the regression tree model
rtree_modl &lt;- rpart(trainSet$av_total ~ ., trainSet)

# model output
rtree_modl</code></pre>
<pre><code>## n= 46922 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 46922 47521.8000 -0.0002743705  
##    2) living_area&lt; 2.643903 45878 20658.9300 -0.0912821900  
##      4) living_area&lt; 0.6747617 38471  5394.6920 -0.2490461000  
##        8) u_full_bth&lt; 0.2953696 31383  2864.0040 -0.3192482000 *
##        9) u_full_bth&gt;=0.2953696 7088  1691.2210  0.0617822800 *
##      5) living_area&gt;=0.6747617 7407  9333.4750  0.7281231000  
##       10) u_int_cnd.E&lt; 1.252425 5591  3986.0670  0.3661451000  
##         20) u_base_floor&lt; 0.03397211 4585  2372.7580  0.1784012000 *
##         21) u_base_floor&gt;=0.03397211 1006   715.1308  1.2218170000 *
##       11) u_int_cnd.E&gt;=1.252425 1816  2359.4090  1.8425610000 *
##    3) living_area&gt;=2.643903 1044  9784.8300  3.9990140000  
##      6) u_int_fin.N&gt;=-2.236257 652  3812.8230  2.6234170000  
##       12) u_int_cnd.E&lt; 1.252425 455  2042.0800  1.9291140000 *
##       13) u_int_cnd.E&gt;=1.252425 197  1044.8180  4.2270110000 *
##      7) u_int_fin.N&lt; -2.236257 392  2686.1840  6.2869960000  
##       14) living_area&lt; 4.533859 292   782.9648  5.2369600000 *
##       15) living_area&gt;=4.533859 100   641.1687  9.3530990000 *</code></pre>
<pre class="r"><code># visualizing decision trees
rpart.plot(rtree_modl, digits = 4, fallen.leaves = TRUE, type = 3, 
    extra = 101)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code># predicting for test dataset
testSet$pred_rtree &lt;- predict(rtree_modl, testSet[, c(1:13)])

# accuracy
cor(testSet$pred_rtree, testSet$av_total)</code></pre>
<pre><code>## [1] 0.8178669</code></pre>
<pre class="r"><code># root mean squared error
RMSE(testSet$pred_rtree, testSet$av_total)</code></pre>
<pre><code>## [1] 0.5642555</code></pre>
<p>The model above predicts the target variable using regression tree. It uses automatic feature selection for the given database. This model when used on the test dataset the predictions of av_total is having correlation of <span class="math inline">\(0.82\)</span> and root mean squared error <span class="math inline">\(0.56\)</span>.</p>
</div>
<div id="artificial-neural-network" class="section level2">
<h2>Artificial Neural Network</h2>
<pre class="r"><code># Loading the required libraries library(&#39;neuralnet&#39;)

# Training the neural network model
neur_modl &lt;- neuralnet(av_total ~ living_area + land_sf + gross_area + 
    u_full_bth + u_int_cnd.E + u_base_floor + u_view.E + u_view.S + 
    u_bdrms + u_bth_style.S + u_kitch_style.S + u_ac.N + u_int_fin.N, 
    trainSet)

# visualising network topology
plot(neur_modl)

# generate predictions on the test dataset
model_results &lt;- compute(neur_modl, testSet[, c(1:13)])

# extracting $net.result for predicted values
testSet$pred_ann &lt;- model_results$net.result

# accuracy
cor(testSet$pred_ann, testSet$av_total)</code></pre>
<pre><code>##             [,1]
## [1,] 0.867929356</code></pre>
<pre class="r"><code># Root mean squared error
RMSE(testSet$pred_ann, testSet$av_total)</code></pre>
<pre><code>## [1] 0.4887411274</code></pre>
<p>Neural network model is used here as a third prediction model for assessment values of condos. It is capable of modeling more complex patterns than nearly any algorithm. This model when used on the test dataset the predictions of av_total is having correlation of <span class="math inline">\(0.87\)</span> and root mean squared error <span class="math inline">\(0.49\)</span>.</p>
</div>
<div id="ensemble-learner" class="section level2">
<h2>Ensemble learner</h2>
<p>Ensembling is a method to put together two or more Machine Learning algorithm learners to make more profound and strong model that includes the effect of all the learners which can improve the accuracy quite high. The ensamble method used here is <strong>stacking</strong>, which means creating multiple models of diffrent type and combining them into a supervisor model.</p>
<p>Here, with the help of <code>caretEnsemble</code> package this implimentation is made very easy, to incorporate the above three models in to one. This ensamble model is <strong>cross validated</strong> with k=5, for better accuracy. For ensamble of linear regression, neural network and regression tree, <strong>genralised linear regression (glm)</strong> model is used. Following is the commented code chunk for the ensamble.</p>
<pre class="r"><code># Loading the required libraries library(&#39;caretEnsemble&#39;)

# Stacking algorithms create submodels

# setting training controls control
control &lt;- trainControl(method = &quot;cv&quot;, number = 5, savePredictions = TRUE)

# crating required algorithms list
algorithmList &lt;- c(&quot;lm&quot;, &quot;rpart&quot;, &quot;nnet&quot;)

# setting seed for reproducibility
set.seed(25)</code></pre>
<pre class="r"><code># Running the models in the list
models &lt;- caretList(av_total ~ ., data = trainSet, trControl = control, 
    methodList = algorithmList)</code></pre>
<pre class="r"><code># for collection, analyzing and visualizing a set of
# resampling results
results &lt;- resamples(models)

# See and visualize results
dotplot(results)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>It is imperative to look at the correlation when the predictions of various models are combined together and used for stacking. It is better when the sub-models have low correlation. In other words, the models are helpful in predicting but in their own diffrent ways. This helps in the the ensambles to extract the best out of each model and improve the accuracy.</p>
<p>Following, shows that the none of the three models are correlated.</p>
<pre class="r"><code># correlation between results
modelCor(results)</code></pre>
<pre><code>##                  lm          rpart           nnet
## lm     1.0000000000 -0.35249827032  0.13343591084
## rpart -0.3524982703  1.00000000000 -0.08254550081
## nnet   0.1334359108 -0.08254550081  1.00000000000</code></pre>
<pre class="r"><code>splom(results)</code></pre>
<p><img src="/about/post/Pro4_Housing/DA5030.Proj.Rawal_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Let’s combine the predictions of the classifiers using a simple linear model.</p>
<pre class="r"><code># stack using glm
stackControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, 
    repeats = 3, savePredictions = TRUE)
set.seed(25)
stack.glm &lt;- caretStack(models, method = &quot;glm&quot;, trControl = stackControl)</code></pre>
<pre class="r"><code># predicting the assesment values using ensamble
Predictions &lt;- predict(stack.glm, testSet[1:13])

# extracting $net.result for predicted values
testSet$pred_ens &lt;- Predictions

# accuracy
cor(testSet$pred_ens, testSet$av_total)</code></pre>
<pre><code>## [1] 0.8501130993</code></pre>
<pre class="r"><code># Root mean squared error
RMSE(testSet$pred_ens, testSet$av_total)</code></pre>
<pre><code>## [1] 0.5164878916</code></pre>
<p>This model when used on the test dataset, the predictions of av_total is having correlation of <span class="math inline">\(0.85\)</span> and root mean squared error <span class="math inline">\(0.5164878916\)</span>.</p>
</div>
<div id="model-performance-comparision" class="section level2">
<h2>Model Performance comparision</h2>
<pre class="r"><code>perform &lt;- data.frame(Models = c(&quot;Linear Regression&quot;, &quot;Regeression tree&quot;, 
    &quot;Neural Network&quot;, &quot;Ensemble&quot;), Correlation = c(cor(testSet$pred_lr, 
    testSet$av_total), cor(testSet$pred_rtree, testSet$av_total), 
    cor(testSet$pred_ann, testSet$av_total), cor(testSet$pred_ens, 
        testSet$av_total)), RMSE = c(RMSE(testSet$pred_lr, testSet$av_total), 
    RMSE(testSet$pred_rtree, testSet$av_total), RMSE(testSet$pred_ann, 
        testSet$av_total), RMSE(testSet$pred_ens, testSet$av_total)), 
    MAE = c(MAE(testSet$pred_lr, testSet$av_total), MAE(testSet$pred_rtree, 
        testSet$av_total), MAE(testSet$pred_ann, testSet$av_total), 
        MAE(testSet$pred_ens, testSet$av_total)))

perform</code></pre>
<pre><code>##              Models  Correlation         RMSE          MAE
## 1 Linear Regression 0.8440260897 0.5259862832 0.3448369596
## 2  Regeression tree 0.8178668660 0.5642555049 0.3511926822
## 3    Neural Network 0.8679293560 0.4887411274 0.2945381109
## 4          Ensemble 0.8501130993 0.5164878916 0.3243139958</code></pre>
<p>The model performance comparision above presents the correlation, root mean squared error and mean absolute error for all four model predictions that we created in this study. The best performer is neural network learner in all three aspects. However, important thing to consider here is the <strong>model tuning of ensamble learner is not carried out, which could have made more diffrence in the prediction</strong>. Ensemble learners are most of the time better at predicting because of the combined effect of diffrent input variables. As we can see here that the Ensemble was able to reduce the RMSE compared to linear regression and regression tree. However, though it is close, it is not as effective as neural network.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<ul>
<li><p>This exploratory research is focused on analysing the Boston property trends and make models to predict property values which can be helpful to sellers and buyers of Boston.</p></li>
<li><p>Data selection is very important part here. It gives direction to the whole process. For example, here it is only “condos”, if there are more categoris of propertise in the analysis, then the data to be selected would be quite diffrent, and eventualy the model prediction would also be diffrent.</p></li>
<li><p>Same process can be applied for other type of property categories, with approprite dataselection and processing, to make prediction models.</p></li>
<li><p>Ensamble learners are not always effective at predicting and lowering the error. However it depends on the tuning parameters applied. In this study tuning was not done.</p></li>
</ul>
</div>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Copyright � 2019 Tejas Rawal. Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="/about/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>